{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vision transformer\n",
    "学习"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T09:50:32.770607Z",
     "start_time": "2024-07-31T09:50:31.603130Z"
    }
   },
   "source": [
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T09:50:32.775295Z",
     "start_time": "2024-07-31T09:50:32.771615Z"
    }
   },
   "source": [
    "class PathEmbed(nn.Module):\n",
    "    \"\"\"2d images to path Embedding \n",
    "    \"\"\"\n",
    "    def __init__(self, image_size = 224, path_size = 16, in_c = 3, embed_dim = 768, norm_layer = None):\n",
    "        super().__init__()\n",
    "        image_size=(image_size, image_size)\n",
    "        path_size = (path_size, path_size)\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = path_size\n",
    "        self.grid_size = (image_size[0] // path_size[0], image_size[1] // path_size[1])\n",
    "        self.num_patches = self.grid_size[0]*self.grid_size[1]\n",
    "\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=path_size, stride=path_size) # 按照默认参数 输出维度是 768 * 14 * 14\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        assert H == self.image_size[0] and W == self.image_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}).\"\n",
    "        \n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C]\n",
    "        x = self.proj(x).flatten(2).transpose(1,2)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    " "
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T09:50:32.779891Z",
     "start_time": "2024-07-31T09:50:32.776288Z"
    }
   },
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim, # 输入token的dim\n",
    "                 num_heads=False,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 proj_drop_ratio=0.\n",
    "                 ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias) # 将x--3倍维度扩张-->q k v\n",
    "        self.atten_drop = nn.Dropout(attn_drop_ratio)\n",
    "        self.proj = nn.Linear(dim,dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,N,C = x.shape # 1，2，6\n",
    "        # self.qkv(x)->1,2,9\n",
    "        # permute 维度置换 3(代表q、k、v),batch 1,head 3, 序列2, 嵌入2\n",
    "        qkv = self.qkv(x).reshape(B,N,3,self.num_heads,\n",
    "                                  C//self.num_heads).permute(2,0,3,1,4) \n",
    "        q,k,v = qkv[0],qkv[1],qkv[2] \n",
    "\n",
    "        attn = (q@k.tanspose(-2,-1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.atten_drop(attn)\n",
    "\n",
    "\n",
    "        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "        \n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T09:50:32.784075Z",
     "start_time": "2024-07-31T09:50:32.781443Z"
    }
   },
   "source": [
    "def drop_path(x, drop_prob:float=0., training:bool=False):\n",
    "    \"\"\"\n",
    "    参考连接: https://blog.csdn.net/qq_43135204/article/details/127912029\n",
    "    \n",
    "    :param x: \n",
    "    :param drop_prob: \n",
    "    :param training: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if drop_prob == 0 or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0], ) + (1, ) * (x.ndim - 1) # x的第一维度,x的维度数 \n",
    "\n",
    "\n",
    "    # 随机均匀的在(0,1]上生成shape形状的tensor每个点加上keep_prob\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    # print(random_tensor)\n",
    "    random_tensor.floor_()# 向下取整操作\n",
    "    # print(random_tensor)\n",
    "    \n",
    "    #将x中的每个元素除以keep_prob（放缩操作） 然后乘以random_tensor 这里巧妙的利用的广播机制\n",
    "    # 意义在于 rt经过向下取整后只有0/1可以忽略为0的位置，这里放缩理解不是很清晰应该是加大存在元素的影响\n",
    "    output = x.div(keep_prob) * random_tensor \n",
    "\n",
    "    return output"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T09:50:32.787151Z",
     "start_time": "2024-07-31T09:50:32.784884Z"
    }
   },
   "source": [
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    每个样本的下降路径（随机深度）（当应用于残余块的主路径时）。\"\"\"\n",
    "    \n",
    "    def __init__(self,drop_prob = None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return drop_path(x,self.drop_prob, self.training)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T09:50:32.835251Z",
     "start_time": "2024-07-31T09:50:32.832452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,act_layer=nn.GELU,drop=0):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T09:50:33.364477Z",
     "start_time": "2024-07-31T09:50:33.360946Z"
    }
   },
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop_ratio=0.,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 drop_path_ratio=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer = nn.LayerNorm\n",
    "                 ):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim,num_heads=num_heads,qkv_bias=qkv_bias,qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_path_ratio\n",
    "                              )\n",
    "        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio >0. else nn.Identity()   \n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp=MLP(in_features=dim,\n",
    "                     hidden_features=mlp_hidden_dim,\n",
    "                     act_layer=act_layer,\n",
    "                     drop=drop_ratio)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T03:18:29.533577Z",
     "start_time": "2024-07-31T03:18:29.531507Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mVisionTransformer\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      2\u001B[0m    \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, image_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m224\u001B[39m, path_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, in_c\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m,\n\u001B[1;32m      3\u001B[0m                 embed_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m768\u001B[39m, depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m12\u001B[39m, num_heads\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m12\u001B[39m,mlp_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4.0\u001B[39m,qkv_bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      4\u001B[0m                 qk_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, representation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, distilled\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,drop_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.\u001B[39m,\n\u001B[1;32m      5\u001B[0m                 attn_drop_ration\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.\u001B[39m, drop_path_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m,embed_layer\u001B[38;5;241m=\u001B[39mPathEmbed,norm_layer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m      6\u001B[0m                 act_layer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m      7\u001B[0m                 ):\n\u001B[1;32m      8\u001B[0m \u001B[38;5;250m       \u001B[39m\u001B[38;5;124;03m\"\"\"_summary_\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03m       Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;124;03m           act_layer (_type_, optional): _description_. Defaults to None.\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124;03m       \"\"\"\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "   def __init__(self, image_size=224, path_size=16, in_c=3, num_classes=1000,\n",
    "                embed_dim=768, depth=12, num_heads=12,mlp_ratio=4.0,qkv_bias=True,\n",
    "                qk_scale=None, representation=None, distilled=False,drop_ratio=0.,\n",
    "                attn_drop_ration=0., drop_path_ratio=0.5,embed_layer=PathEmbed,norm_layer=None,\n",
    "                act_layer=None\n",
    "                ):\n",
    "       \"\"\"_summary_\n",
    "\n",
    "       Args:\n",
    "           image_size (int, optional): 图片输入的size.\n",
    "           path_size (int, optional): 图片每个path的size.\n",
    "           in_c (int, optional): 图片输入的通道数.\n",
    "           number_classes (int, optional):分类数\n",
    "           embed_dim (int, optional): 嵌入维度\n",
    "           depth (int, optional): transformer 深度 ？\n",
    "           num_heads (int, optional): 注意力头数\n",
    "           mlp_ratio (float, optional): mlp嵌入跟词嵌入的维度比例\n",
    "           qkv_bias (bool, optional): 是否需要qkv权重的偏置项\n",
    "           qk_scale (_type_, optional): _description_. Defaults to None.\n",
    "           representation (_type_, optional): _description_. Defaults to None.\n",
    "           distilled (bool, optional): _description_. Defaults to False.\n",
    "           drop_ratio (_type_, optional): drop指标.\n",
    "           attn_drop_ration (_type_, optional): _description_. Defaults to 0..\n",
    "           drop_path_ratio (float, optional): _description_. Defaults to 0.5.\n",
    "           embed_layer (_type_, optional): _description_. Defaults to PathEmbed.\n",
    "           norm_layer (_type_, optional): _description_. Defaults to None.\n",
    "           act_layer (_type_, optional): _description_. Defaults to None.\n",
    "       \"\"\"\n",
    "       super(VisionTransformer, self).__init__()\n",
    "       self.num_classes = num_classes\n",
    "       self.num_features = self.embed_dim=embed_dim\n",
    "        \n",
    "       self.num_tokens = 2 if distilled else 1\n",
    "       norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "       act_layer = act_layer or nn.GELU\n",
    "\n",
    "       self.patch_embed = embed_layer(image_size=image_size, path_size=path_size, in_c=in_c, embed_dim=embed_dim)\n",
    "       num_patches = self.patch_embed.num_patches\n",
    "\n",
    "       self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "       self.dist_token = nn.Parameter(torch.zeros(1,1,embed_dim)) if distilled else None\n",
    "       self.pos_embed = nn.Parameter(torch.zeros(1, num_patches+self.num_tokens, embed_dim))\n",
    "       self.pos_drop = nn.Dropout(p=drop_ratio)\n",
    "\n",
    "       dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)] # stochastic depth decay rule 随机深度衰减规则\n",
    "       self.blocks = nn.Sequential(*[\n",
    "           Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, )\n",
    "       ])\n",
    "\n",
    "\n",
    "       \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchv01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
