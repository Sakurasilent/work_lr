{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vision transformer\n",
    "学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathEmbed(nn.Module):\n",
    "    \"\"\"2d images to path Embedding \n",
    "    \"\"\"\n",
    "    def __init__(self, image_size = 224, path_size = 16, in_c = 3, embed_dim = 768, norm_layer = None):\n",
    "        super().__init__()\n",
    "        image_size=(image_size, image_size)\n",
    "        path_size = (path_size, path_size)\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = path_size\n",
    "        self.grid_size = (image_size[0] // path_size[0], image_size[1] // path_size[1])\n",
    "        self.num_patched = self.grid_size[0]*self.grid_size[1]\n",
    "\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=path_size, stride=path_size) # 按照默认参数 输出维度是 768 * 14 * 14\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,C,H,W = x.shape\n",
    "        assert H == self.image_size[0] and W == self.image_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}).\"\n",
    "        \n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C]\n",
    "        x = self.proj(x).flatten(2).transpose(1,2)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "   def __init__(self, image_size=224, path_size=16, in_c=3, num_classes=1000,\n",
    "                embed_dim=768, depth=12, num_heads=12,mlp_ratio=4.0,qkv_bias=True,\n",
    "                qk_scale=None, representation=None, distilled=False,drop_ratio=0.,\n",
    "                attn_drop_ration=0., drop_path_ratio=0.5,embed_layer=PathEmbed,norm_layer=None,\n",
    "                act_layer=None\n",
    "                ):\n",
    "       \"\"\"_summary_\n",
    "\n",
    "       Args:\n",
    "           image_size (int, optional): 图片输入的size.\n",
    "           path_size (int, optional): 图片每个path的size.\n",
    "           in_c (int, optional): 图片输入的通道数.\n",
    "           number_classes (int, optional):分类数\n",
    "           embed_dim (int, optional): 嵌入维度\n",
    "           depth (int, optional): transformer 深度 ？\n",
    "           num_heads (int, optional): 注意力头数\n",
    "           mlp_ratio (float, optional): mlp嵌入跟词嵌入的维度比例\n",
    "           qkv_bias (bool, optional): 是否需要qkv权重的偏置项\n",
    "           qk_scale (_type_, optional): _description_. Defaults to None.\n",
    "           representation (_type_, optional): _description_. Defaults to None.\n",
    "           distilled (bool, optional): _description_. Defaults to False.\n",
    "           drop_ratio (_type_, optional): drop指标.\n",
    "           attn_drop_ration (_type_, optional): _description_. Defaults to 0..\n",
    "           drop_path_ratio (float, optional): _description_. Defaults to 0.5.\n",
    "           embed_layer (_type_, optional): _description_. Defaults to PathEmbed.\n",
    "           norm_layer (_type_, optional): _description_. Defaults to None.\n",
    "           act_layer (_type_, optional): _description_. Defaults to None.\n",
    "       \"\"\"\n",
    "       super(VisionTransformer, self).__init__()\n",
    "       self.num_classes = num_classes\n",
    "       self.num_features = self.embed_dim=embed_dim\n",
    "\n",
    "       self.num_tokens = 2 if distilled else 1\n",
    "       norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchv01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
